{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic string breaker \"word word2\" -> ['word', 'word2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123', 'alpha', 'alpha123', 'c/o', 'alpha', 'STREET']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def w(str_sentence):\n",
    "    return re.findall(r\"[\\w'/-:]+|[.,!?;]\", str_sentence)\n",
    "st = w(\"123 alpha alpha123 c/o alpha STREET\")\n",
    "st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder, takes a single word and returns a list of 'features' characterizing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123', 'south', 'main', 'street', '.']\n",
      "[['digit'], ['dir', 'alpha'], ['alpha'], ['alpha', 'way'], ['period']]\n",
      "\n",
      "['1217', 'iris', 'box', '222']\n",
      "[['digit'], ['alpha'], ['alpha', 'pob2'], ['digit']]\n",
      "\n",
      "['1217', 'iris', 'box']\n",
      "[['digit'], ['alpha'], ['alpha', 'pob2']]\n",
      "\n",
      "['1470', 'la', 'cima', 'rd']\n",
      "[['digit'], ['alpha'], ['alpha'], ['alpha', 'way']]\n",
      "\n",
      "['1217', 'iris', 'court', 'c/o', 'stewy', 'dewy']\n",
      "[['digit'], ['alpha'], ['alpha'], ['deleg'], ['alpha'], ['alpha']]\n",
      "\n",
      "['1215', 'iris', 'court', 'c/o', 'stewy']\n",
      "[['digit'], ['alpha'], ['alpha'], ['deleg'], ['alpha']]\n",
      "\n",
      "['123', 'elcrest', 'hwy']\n",
      "[['digit'], ['alpha'], ['alpha', 'way']]\n",
      "\n",
      "['2323', 'elmhurst', 'way']\n",
      "[['digit'], ['alpha'], ['alpha']]\n",
      "\n",
      "['c/o', 'frank', '123', 'south', 'main', 'st', '.']\n",
      "[['deleg'], ['alpha'], ['digit'], ['dir', 'alpha'], ['alpha'], ['alpha', 'pre', 'way'], ['period']]\n",
      "\n",
      "['123', 'main', 'st', '.', 'attn:', 'elmer', 'fudge']\n",
      "[['digit'], ['alpha'], ['alpha', 'pre', 'way'], ['period'], ['deleg'], ['alpha'], ['alpha']]\n",
      "\n",
      "['333', 'st', 'james', 'st']\n",
      "[['digit'], ['alpha', 'pre', 'way'], ['alpha'], ['alpha', 'pre', 'way']]\n",
      "\n",
      "['c/o', 'forever', '21', '111', 'south', 'broadway']\n",
      "[['deleg'], ['alpha'], ['digit'], ['digit'], ['dir', 'alpha'], ['alpha']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def encoder(word):\n",
    "    encodings = {\n",
    "        'alpha': [r'^[a-z]+$'],\n",
    "        'digit': [r'^\\d+$'],\n",
    "        'alnum': [r'^(\\d+[a-z]+|[a-z]+\\d+)[\\da-z]*$'],\n",
    "        'comma': [r'^,$'],\n",
    "        'period': [r'^\\.$'],\n",
    "        'way': [r'^street$', r'^st$',r'^road$',r'^rd$', r'^ave$',r'^avenue$',r'^hwy$', r'^highway$'],\n",
    "        'deleg': [r'^attn$', r'^attn:$', r'^c\\/o$', r'^co$' ],\n",
    "        'pob0': [r'^po$', r'^p\\.o\\.$'],\n",
    "        'pob2': [r'^box$'],\n",
    "        'dir':  [r'^east$',r'^west$',r'^north$',r'^south$',],\n",
    "        'pre':  [r'^st$']\n",
    "    \n",
    "    }\n",
    "    hits = []\n",
    "    for key, rexs in encodings.items():\n",
    "        for rex in rexs:\n",
    "            if re.match(rex, word):\n",
    "                hits.append(key)\n",
    "    return hits\n",
    "\n",
    "addresses = [\n",
    "    '123 south main street.',\n",
    "    '1217 iris box 222',\n",
    "    '1217 iris box',\n",
    "    '1470 la cima rd',\n",
    "    '1217 iris court c/o stewy dewy',\n",
    "    '1215 iris court c/o stewy',\n",
    "    '123 elcrest hwy',\n",
    "    '2323 elmhurst way',\n",
    "    'c/o frank 123 south main st.',\n",
    "    '123 main st. attn: elmer fudge',\n",
    "    '333 st james st',\n",
    "    'c/o forever 21 111 south broadway'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for address in addresses:\n",
    "    print(w(address))\n",
    "    print([encoder(word) for word in w(address)])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The lexicon and get_normalized fn determine how to intepret a word.\n",
    "* So for example, 'st', can mean 'saint', or 'street', depending if we look at it in the context of 'pre'(prefix) or 'way' as in a street or road designation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saint\n",
      "street\n"
     ]
    }
   ],
   "source": [
    "lexicon = {\n",
    "        'way': {\n",
    "            'street': ['st', 'street'],\n",
    "            'road':  ['rd', 'road'],\n",
    "            'avenue': ['ave', 'avenue']\n",
    "            },\n",
    "        'pre': {\n",
    "            'saint': ['st']\n",
    "        }\n",
    "}\n",
    "\n",
    "def get_normalized(word, _way, lexicon):\n",
    "    for entry, lst_matches in lexicon[_way].items():\n",
    "        if word in lst_matches:\n",
    "            return entry\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(get_normalized('st', 'pre', lexicon))\n",
    "print(get_normalized('st', 'way', lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps...\n",
    "* Combine encoded sentence with a predetermined pattern, say like expecting 'way' type of words at end and see if we can reliably interpret the meaning of abbrevs like st.\n",
    "* Automate geneation of the address templates by running encoder on a looooong list of valid addresses\n",
    "* Hopefully after that an address can be normalized reliably... and this can then be applied maybe to business names....\n",
    "\n",
    "* and IF that works, ^^, then do it for 'c/o', 'attn:', and 'po box' phrases... and the write more code to break up a long sentence into its composite phrases.  So like \"some very long string with lots of words\" => \"c/o substring\" + \"valid address\" + \"junk on end\".....\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
